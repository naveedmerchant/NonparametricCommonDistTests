\documentclass[11pt]{article}
\usepackage{color}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{etaremune}
\usepackage{soul}

% Page height, width and margins
\textwidth=18cm
\textheight=22cm % this gives 2 additional lines in double spaced document
\oddsidemargin=-20pt
\topmargin=-20pt



% Font for the whole document	
\usepackage[bitstream-charter]{mathdesign}
\usepackage[T1]{fontenc}
\usepackage{amsmath}



% Mini formatting things
% Data to the most right of the page
\newcommand{\when}[1]{
\textit{\textbf{#1}}
}

\begin{document}

We first make the following definitions:

$X = [x_1,x_2,...,x_n]$

$Y = [y_1,y_2,...,y_m]$

$X = [X_T,X_V]$

$Y = [Y_T,Y_V]$

That is X and Y are composed of observations, and each can be partitioned into training and validation sets.

Define $f(X_v; X_t |h) \equiv \prod_{j \in X_v} \frac{1}{kh} \sum_{i \in X_t} K(\frac{j-i}{h})$

Where $k \equiv $ the number of elements in $X_t$

Recall our problem is we want to compute:

BF  $\equiv \frac{\int_0^\infty \! f([X_V,Y_V];[ X_T,Y_T] |h) f_1(h)\, \mathrm{d}h}{\int_0^\infty \! f(Y_V; Y_T|h) f_3(h)\, \mathrm{d}h \int_0^\infty \! f(X_V; X_T|h) f_2(h)\, \mathrm{d}h}$

and show this goes to 0 under some conditions and infinity in some other conditions.

We know a closed form for these integrals for a particular choice of $f_1, f_2, f_3$ and the Gaussian Kernel.

Thus we can say the BF is also equal to:

= $(\frac{1}{(k_1 + k_2)\sqrt{2\pi}})^{n_1+n_2} \frac{B_1}{\sqrt{\pi}}  \sum_{i_{n_1+n_2} \in [X_T, Y_T]} \sum_{i_{n_1+n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} \frac{ \Gamma (\frac{n_1 + n_2 -1}{2})}{(.5(2B_1^2 +  \sum_{L=1}^{n_1+n_2} ([X_V, Y_V]_{L} - i_L)^2))^{\frac{(n_1+n_2)-1}{2}}} $ divided by

$(\frac{1}{(k_1)\sqrt{2\pi}})^{n_1} \frac{B_2}{\sqrt{\pi}}  \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]} \frac{ \Gamma (\frac{n_1-1}{2})}{(.5(2B_2^2 +  \sum_{L=1}^{{n_1}} ([X_V]_{L} - i_L)^2))^{\frac{{n_1}-1}{2}}}  * $


$ (\frac{1}{(k_2)\sqrt{2\pi}})^{n_2} \frac{B_3}{\sqrt{\pi}}  \sum_{i_{n_2} \in [Y_T]} \sum_{i_{n_2-1}\in [Y_T]} \ldots  \sum_{i_{2}\in [Y_T]} \sum_{i_{1}\in [Y_T]} \frac{ \Gamma (\frac{n_2-1}{2})}{(.5(2B_3^2 +  \sum_{L=1}^{n_2} ([Y_V]_{L} - i_L)^2))^{\frac{n_2-1}{2}}} $

Where 

$k_1 \equiv $ the number of elements in $X_T$

$k_2 \equiv $ the number of elements in $Y_T$

$n_1 \equiv $ the number of elements in $X_V$

$n_2 \equiv $ the number of elements in $Y_V$


When we say $[Y_V]_{L}$, we mean $Y_V$ is a set of elements, and we arbitrarily index them.

This can be simplified partially into:

$\frac{k_1^{n_1}k_2^{n_2}}{(k_1 + k_2)^{n_1+n_2}} \frac{B_1}{B_2 B_3 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} $ 

$ \sum_{i_{n_1+n_2} \in [X_T, Y_T]} \sum_{i_{n_1+n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} \frac{1}{(.5(2B_1^2 +  \sum_{L=1}^{n_1+n_2} ([X_V, Y_V]_{L} - i_L)^2))^{\frac{(n_1+n_2)-1}{2}}} $ divided by 

$ \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]} \frac{1}{(.5(2B_2^2 +  \sum_{L=1}^{{n_1}} ([X_V]_{L} - i_L)^2))^{\frac{{n_1}-1}{2}}}  * $

$ \sum_{i_{n_2} \in [Y_T]} \sum_{i_{n_2-1}\in [Y_T]} \ldots  \sum_{i_{2}\in [Y_T]} \sum_{i_{1}\in [Y_T]} \frac{1}{(.5(2B_3^2 +  \sum_{L=1}^{n_2} ([Y_V]_{L} - i_L)^2))^{\frac{n_2-1}{2}}} $

$\mathbf{B}$ refers to the Beta function. This should just be a generalized Binomial coefficient.

We can view the sums with their $k^n$ terms as averages instead of sums. Then the big sum with the constant can be interpreted as the average of the euclidean distance from the training set to the validation set over all possible choices of the training set if I drew them with replacement, with mild pertubation from the prior.

For now because of the assumptions of our test, $[X_V]_{L}$ is the same distribution as $i_L$.

Define $f \equiv 1 / (B_1^2 + .5n*(x^2)$ where x is a random variable drawn from this distribution $[X_V]_{L} - i_L$

The taylor series for f(x) is: $\sum_{D=0}^\infty \frac{(.5n)^{D}x^{2D}}{B_1^{2D+1}}$ if $|x| < \sqrt{\frac{B_1}{.5n}} $

if $|x| > \sqrt{\frac{B_1}{.5n}}$ the appropriate taylor series is: $\sum_{D=1}^\infty \frac{(.5n)^{-D}x^{-2D}}{B_1^{2*(D-1)}} \equiv g$ 

Applying expectation to the first implies: 

$E[f(x)] = \sum_{D=0}^\infty \frac{(.5n)^D E[x^{2D}]}{B_1^{2D+2}}$

The second is NOT dangerous. While the negative moments are normally not defined, since we are looking at a variable that is 0 for a large interval around 0, all the negative moments are infact GURANTEED to be defined. I will have to think about this a bit more later... This is good however, we no longer need to restrict the prior at all.

Applying expectation to the second implies:

$E[g(x)] = \sum_{D=1}^\infty \frac{(.5n)^{-D}E[x^{-2D}]}{B_1^{2*(D-1)}}$

There's actually a similar story with the positive moments as well. I am under the impression all positive moments should exist because these are truncated moments, and not actual moments. Even the Cauchy distribution's moments should be finite, as this isn't integrating from -infinity to infinity. For now only examine the case where $|x| < \frac{B_1}{.5n} $.

Define X and Y to be from the same distribution and independent then:

$E[x^{2D}] = E[(X-Y)^{2D}] = \sum_{i=0}^{2D} \binom{2D}{i}(-1)^iE[X^{2D-i}]E[X^{i}]$

So...
\begin{obeylines}
$ \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]} \frac{1}{(.5(2B_2^2 +  \sum_{L=1}^{{n_1}} ([X_V]_{L} - i_L)^2))^{\frac{{n_1}-1}{2}}}   $  

Should be the same as:

$ \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]} (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_1-1}{2}}   $ 

\end{obeylines}

Similarly:

$ \sum_{i_{n_2} \in [Y_T]} \sum_{i_{n_2-1}\in [Y_T]} \ldots  \sum_{i_{2}\in [Y_T]} \sum_{i_{1}\in [Y_T]} \frac{1}{(.5(2B_3^2 +  \sum_{L=1}^{n_2} ([Y_V]_{L} - i_L)^2))^{\frac{n_2-1}{2}}} $ should be the same as:

$ \sum_{i_{n_2} \in [Y_T]} \sum_{i_{n_2-1}\in [Y_T]} \ldots  \sum_{i_{2}\in [Y_T]} \sum_{i_{1}\in [Y_T]} (\sum_{D=0}^\infty \frac{(.5n_2)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[Y^{i}]}{B_2^{2D+2}})^{\frac{n_2-1}{2}}   $ 

Finally: 

$ \sum_{i_{n_1+n_2} \in [X_T, Y_T]} \sum_{i_{n_1+n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} \frac{1}{(.5(2B_1^2 +  \sum_{L=1}^{n_1+n_2} ([X_V, Y_V]_{L} - i_L)^2))^{\frac{(n_1+n_2)-1}{2}}} $

If distribution of X and Y are the same then:

$ \sum_{i_{n_1 + n_2} \in [X_T, Y_T]} \sum_{i_{n_1 + n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} (\sum_{D=0}^\infty \frac{((.5(n_1 + n_2))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[X^{i}]}{B_3^{2D+2}})^{\frac{n_1 + n_2 -1}{2}}  $ 

Earlier we said that we could treat these sums like means, then:

$ \frac{1}{(k_1+k_2)^{n_1+n_2}} \sum_{i_{n_1 + n_2} \in [X_T, Y_T]} \sum_{i_{n_1 + n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} \sum_{D=0}^\infty \frac{((.5(n_1 + n_2))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[X^{i}]}{B_3^{2D}}  $ should be the same as 

$(\sum_{D=0}^\infty \frac{((.5(n_1 + n_2))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[X^{i}]}{B_3^{2D+2}})^{\frac{n_1+n_2 - 1}{2}} $

Alternatively this is true because the term inside our sum no longer depends on $i_l$ so it is the same as adding a constant many times and then dividing by the number of times we summed.

Similarly:

 $ \frac{1}{(k_2)^{n_2}} \sum_{i_{n_2} \in [Y_T]} \sum_{i_{n_2-1}\in [Y_T]} \ldots  \sum_{i_{2}\in [Y_T]} \sum_{i_{1}\in [Y_T]} (\sum_{D=0}^\infty \frac{(.5n_2)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[Y^{i}]}{B_2^{2D+2}})^{\frac{n_2 - 1}{2}}    $

is the same as:

$(\sum_{D=0}^\infty \frac{(.5n_2)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[Y^{i}]}{B_2^{2D+2}})^{\frac{n_2 - 1}{2}} $

and 

 $ \frac{1}{(k_1)^{n_1}} \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]}  (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_1 - 1}{2}}   $

is around: 

$(\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_1 - 1}{2}}$ 

Our BF under the Null then is roughly:

$\frac{B_1}{B_2 B_3 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} * $

$\sum_{D=0}^\infty \frac{((.5(n_1 + n_2))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[X^{i}]}{B_3^{2D+2}}$ divided by 


$ (\sum_{D=0}^\infty \frac{(.5n_2)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[Y^{2D-i}]E[Y^{i}]}{B_2^{2D+2}}) * (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}}) $

The $B_i$'s are from the prior, and is the IQR, if the distributions are the same we expect them to be similar, so... the BF under null should be

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} * $

$(\sum_{D=0}^\infty \frac{((.5(n_1 + n_2))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_1+n_2 - 1}{2}}$ divided by 


$ (\sum_{D=0}^\infty \frac{(.5n_2)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_2 - 1}{2}} * (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{n_1 - 1}{2}} $

For simplicity we show first what happens when $n_1 = n_2$

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_1 -1}{2})} * $

$(\sum_{D=0}^\infty \frac{(((n_1))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_1- .5}$ divided by 

$ (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_1- 1} $

An obvious inequality is: $(\sum_{D=0}^\infty \frac{(((n_1))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_1- .5} > (\sum_{D=0}^\infty \frac{(.5n_1)^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_1- .5} $

So this part of the BF is lowerbounded by:

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_1 -1}{2})} * (\sum_{D=0}^\infty \frac{(((n_1))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{.5} $

Recall the following definitions...

<<<<<<< HEAD
$f \equiv 1 / (B_1^2 + .5n*(x^2)$ where x is a random variable drawn from this distribution $[X_V]_{L} - i_L$ and $|x| < \sqrt{\frac{B_1}{.5n}}$  
=======
$f \equiv 1 / (B_1^2 + .5n*(x^2)$ where x is a random variable drawn from this distribution $[X_V]_{L} - i_L$ and $|x| < \frac{B_1}{.5n}$  
>>>>>>> 24142af350f547237b77d138c33fc65243f453a5

$E[f(x)] = \sum_{D=0}^\infty \frac{(.5n)^D \sum_{i=0}^{2D} \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}}$

Then the sum is similar to $E[f(x)]$ where $f \equiv 1 / (B_1^2 + n_1*(x^2)$ where x is a random variable drawn from this distribution $[X_V]_{L} - i_L$

Clearly, $E[f(x)] > 0$ because $f(x) > 0$

Whatever the case, for the lower bound, the B term rises exponentially with n, and the sum, at worst, decreases quadratically with n. So the square root of the sum should decrease at least linearly with n. As a result, the lowerbound for this BF should go to infinity exponentially. Which implies the BF itself should also go to infinity at least exponentially under the null hypothesis.

This method works if $|x| < \sqrt{\frac{B_1}{.5n}} $. 

For the other case (or rather if $|x| > \sqrt{\frac{B_1}{.5n}} $:

Define X and Y to be from the same distribution and independent then:

$E[x^{-2D}] = E[(X-Y)^{-2D}]$

We are aware it exists for all n I believe. We should be careful asymptotically...

Note that: $\frac{1}{X-Y} = \frac{1}{X-Y} I_{X > Y} + \frac{1}{X-Y} I_{X < Y}$. 

If $|X| > Y$ Then note that:

$(X - Y)^{-n} = \sum_{k=0}^{\infty} (-1)^{2k} \binom{n+k-1}{k}{Y^k}{X^{-n-k}}$

Taking expectations implies:

$E[(X - Y)]^{-n} = \sum_{k=0}^{\infty} (-1)^{2k} \binom{n+k-1}{k}E[{Y^k}]E[{X^{-n-k}}]$

Unfortunately, we now must make the assumption that the moments of X exist. I think it is possible to complete this proof without using this since we know the negative expectations of x do exist, but I am having trouble relating them to the moments of X and Y without using a trick like this.

For the better or worse, the existence of the expectation to a power does not imply anything about whether X or Y have moments that are finite.

I am aware the series converges regardless of whether the expectation is finite, but I am not aware what its form is unless I assume something about its expectation... There's probably something out there that makes this easier...

Similarly:

If $|\frac{X}  {Y} | < 1$ then:

$(X - Y)^{-n} = \sum_{k=0}^{\infty} (-1)^{2k} \binom{n+k-1}{k}{X^k}{Y^{-n-k}}$

Taking expectations and setting n= 2D implies:

$E[(X - Y)]^{-2D} = \sum_{k=0}^{\infty} (-1)^{2k} \binom{2D+k-1}{k}E[{X^k}]E[{Y^{-2D-k}}]$

We now proceed similarly to the previous part:

$E[g(x)] = \sum_{D=1}^\infty \frac{(.5n)^{-D}\sum_{k=0}^{\infty} (-1)^{2k} \binom{2D+k-1}{k}E[{X^k}]E[{Y^{-2D-k}}]}{B_1^{2*(D-1)}} $

\begin{obeylines}
$ \sum_{i_{n_1} \in [X_T]} \sum_{i_{n_1-1}\in [X_T]} \ldots  \sum_{i_{2}\in [X_T]} \sum_{i_{1}\in [X_T]} (\sum_{D=1}^\infty \frac{(.5n)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{Y^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1}-1}{2}}   $  

Again recall, this is like an average:

$(\sum_{D=1}^\infty \frac{(.5n)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{Y^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1}-1}{2}}   $

Our BF should end up being:

$\frac{B_1}{B_2B_3 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} * $

$(\sum_{D=1}^\infty \frac{(.5(n_1+n_2))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{Y^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1 + n_2}-1}{2}} $ divided by

$((\sum_{D=1}^\infty \frac{(.5n_1)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{Y^k}]E[{Y^{-2D-k}}]}{B_2^{2*(D-1)}})^{\frac{{n_1}-1}{2}}  *(\sum_{D=1}^\infty \frac{(.5n_2)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_3^{2*(D-1)}})^{\frac{{n_2}-1}{2}})   $

Suppose null hypothesis is true (distributions are the same) and then that sample sizes are equal, then our $B_i$'s are all equal, and the BF is:

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} * $

$(\sum_{D=1}^\infty \frac{((n_1))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{2*n_1}-1}{2}} $ divided by

$((\sum_{D=1}^\infty \frac{(.5n_1)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1}-1}{2}}  *(\sum_{D=1}^\infty \frac{(.5n_1)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1}-1}{2}})   $

Which is the identical to:

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_1 -1}{2}, \frac{n_2 -1}{2})} * $

$(\sum_{D=1}^\infty \frac{((n_1))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{2*n_1}-1}{2}} $ divided by

$(((\sum_{D=1}^\infty \frac{(.5n_1)^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_1}-1}{2}})^2  $

By an argument identical to the case where $|x| <\sqrt{\frac{B_1}{.5n}}$, we conclude this also should go to infinity at least exponentially (we use the same lower bound, and the same idea to show the sum is positive and less influential then the exponential portion). 

The proof for if $|\frac{Y}  {X} | < 1$ follows in the exact same fashion.

So we use both ideas together now.

Recall our problem is we want to compute:

$\frac{\int_0^\infty \! f([X_V,Y_V];[ X_T,Y_T] |h) f_1(h)\, \mathrm{d}h}{\int_0^\infty \! f(Y_V; Y_T|h) f_3(h)\, \mathrm{d}h \int_0^\infty \! f(X_V; X_T|h) f_2(h)\, \mathrm{d}h}$

We can split each of the integrals into parts where the dataset has a value smaller than $B_i$ and a value larger than $B_i$. For instance for the top integral...

$\int_0^\infty \! f([X_V,Y_V]I_{i \in {X_V,Y_V} and < B_1};[ X_T,Y_T] |h) f_1(h) + \int_0^\infty \! f([X_V,Y_V]I_{i \in {X_V,Y_V} and i > B_1};[ X_T,Y_T] |h) f_1(h)$

We do this for every integral then the BF is:

$\frac{\int_0^\infty \! f([X_V,Y_V];[ X_T,Y_T] |h) f_1(h)\, \mathrm{d}h}{\int_0^\infty \! f(Y_V; Y_T|h) f_3(h)\, \mathrm{d}h \int_0^\infty \! f(X_V; X_T|h) f_2(h)\, \mathrm{d}h} = $

$\frac{\int_0^\infty \! f([X_V,Y_V]I_{i \in {X_V,Y_V} and < \sqrt{\frac{B_1}{.5n}}};[ X_T,Y_T] |h) f_1(h) + \int_0^\infty \! f([X_V,Y_V]I_{i \in {X_V,Y_V} and i > \sqrt{\frac{B_1}{.5n}}};[ X_T,Y_T] |h) f_1(h)}{\int_0^\infty \! f([X_V]I_{i \in {X_V} and < \sqrt{\frac{B_2}{.5n}}};[ X_T|h) f_2(h) + (\int_0^\infty \! f(X_V]I_{i \in {X_V} and i > \sqrt{\frac{B_2}{.5n}}};[X_T] |h) f_2(h)) * (\int_0^\infty \! f([Y_V]I_{i \in {Y_V} and < \sqrt{\frac{B_3}{.5n}}};[ Y_T|h) f_3(h) + \int_0^\infty \! f(Y_V]I_{i \in {Y_V} and i > \sqrt{\frac{B_3}{.5n}}};[Y_T] |h) f_3(h))}$

Assume the following: All expectations are finite, and this is the Null hypothesis so the distributions are the same and that we assign a prior of a similar form to earlier. For additional simplicity assume sample sizes are the same... The BF should be equal to:

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_{1} -1}{2}, \frac{n_1 -1}{2})} * (\sum_{D=1}^\infty \frac{((n_{11}))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{n_{11} - .5} + (\sum_{D=0}^\infty \frac{((n_{12}))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_{12}- .5} $

divided by the product of 

$ ({(\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{21}} - 1}{2}} + (\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{22}} - 1}{2}}})$

and

$({(\sum_{D=1}^\infty \frac{(.5n_{31})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{31}} - 1}{2}} + (\sum_{D=0}^\infty \frac{(.5n_{32})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{32}} - 1}{2}}}) $

We make the following definitions:

$n_{11} + n_{12} = n$

$n_{31} + n_{21} = n_{11}$

$n_{32} + n_{22} = n_{12}$

$n_{31} + n_{32} = .5 * n = n_{21} + n_{22}$

We expand the denominator:

$ (\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{21}} - 1}{2}} * (\sum_{D=1}^\infty \frac{(.5n_{31})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{31}} - 1}{2}} +$ 

$ (\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{22}} - 1}{2}}*(\sum_{D=0}^\infty \frac{(.5n_{32})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{32}} - 1}{2}} +$

$ (\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{22}} - 1}{2}}*(\sum_{D=1}^\infty \frac{(.5n_{31})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{31}} - 1}{2}} +$

$ (\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{21}} - 1}{2}}*(\sum_{D=0}^\infty \frac{(.5n_{32})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{32}} - 1}{2}}$

Recall the numerator is:
$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_{1} -1}{2}, \frac{n_1 -1}{2})} * (\sum_{D=1}^\infty \frac{((n_{11}))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{.5*n_{11} - .5} + (\sum_{D=0}^\infty \frac{((n_{12}))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{.5*n_{12}- .5} $

For now we assume $n_{21} = n_{31} $ and $n_{22} = n_{32}$. Then this simplifies some of the expressions in the denominator and numerator...

Numerator becomes:

$\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_{1} -1}{2}, \frac{n_1 -1}{2})} * (\sum_{D=1}^\infty \frac{((n_{21}))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{n_{21} - .5} + (\sum_{D=0}^\infty \frac{((n_{22}))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_{22}- .5} $

Denominator is:

$ (\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{{n_{21}} - 1} +(\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{{n_{22}} - 1} $

and cross terms... 

We crudely bound the cross terms by taking the square of the higher amount, instead of computing the cross product. This should give an upper bound for denominator, or a lower bound for the BF...

Without loss of generality, suppose 

$ (\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{\frac{{n_{22}} - 1}{2}} > (\sum_{D=1}^\infty \frac{(.5n_{31})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{31}} - 1}{2}} $ and

$ (\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{\frac{{n_{21}} - 1}{2}} > (\sum_{D=0}^\infty \frac{(.5n_{32})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{{{n_{32}} - 1}}$

Then the cross terms are upper bounded by:

$ \sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{{{n_{21}} - 1}} +  \sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{{{n_{22}} - 1}} $

Then the BF is lower bounded by: $ 2(\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{{n_{21}} - 1} + 2(\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{{n_{22}} - 1}$

It may seem the choice of which cross term is larger is of importance... but it only really changes the constant the sums in the denominator is multiplied by.

A lower bound for the BF that we want to examine then is:

 $\frac{1}{B_1 \sqrt{\pi}} * \frac{1}{ \mathbf{B}(\frac{n_{1} -1}{2}, \frac{n_1 -1}{2})} * \frac{ (\sum_{D=1}^\infty \frac{((n_{21}))^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{n_{21} - .5} + (\sum_{D=0}^\infty \frac{((n_{22}))^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{n_{22}- .5}}{2(\sum_{D=1}^\infty \frac{(.5n_{21})^{-D}\sum_{k=0}^{\infty} \binom{2D+k-1}{k}E[{X^k}]E[{X^{-2D-k}}]}{B_1^{2*(D-1)}})^{{n_{21}} - 1} + 2(\sum_{D=0}^\infty \frac{(.5n_{22})^D \sum_{i=0}^{2D} (-1)^i \binom{2D}{i}E[X^{2D-i}]E[X^{i}]}{B_1^{2D+2}})^{{n_{22}} - 1}}$

I think this can be shown to behave as we desire asymptotically by the results we showed in the case where looked at the BF where we restricted whether we examined the x's that were less than a certain quantity and greater than a certain quantity?

\end{obeylines}

Under the alternate we would have to split up the sum into parts where we subtract over the same and different parts. That is introduce indicators.

If two distributions are different it'll show in their moments! So things will behave very differently compared to the null situation.

$ \sum_{i_{n_1+n_2} \in [X_T, Y_T]} \sum_{i_{n_1+n_2-1}\in [X_T, Y_T]} \ldots  \sum_{i_{2}\in [X_T, Y_T]} \sum_{i_{1}\in [X_T, Y_T]} \frac{1}{(.5(2B_1^2 +  \sum_{L=1}^{n_1+n_2} ([X_V, Y_V]_{L}*I_{([X_V, Y_V]_{L} \in X_V)} - i_L)^2))^{\frac{(n_1+n_2)-1}{2}}} $

Perhaps there is an easier way to deal with this without creating so many terms...

\end{document}
